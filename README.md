# Huang Lab - Enzyme Kinetic Parameter Prediction

A comprehensive machine learning framework for predicting enzyme kinetic parameters (kcat, Km, Ki, Kd) using various regression models and neural networks.

> this readme file is generated by claude 4


## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Datasets](#datasets)
- [Models](#models)
- [Installation](#installation)
- [Usage](#usage)
- [Experiments](#experiments)
- [Results](#results)
- [Contributing](#contributing)
- [References](#references)

## ğŸ”¬ Overview

This repository contains machine learning experiments for predicting enzyme kinetic parameters using various approaches including:

- Traditional machine learning models (SVR, Random Forest, XGBoost, etc.)
- Deep learning models (CNN, MLP, Transformer)
- Comparison with state-of-the-art methods (CatPred, UniKP, DLKcat, EITLEM)

The project focuses on different data splitting strategies including cold protein and cold molecule splits to evaluate model generalization capabilities.

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ dataset_catpred_kcat/        # CatPred kcat experiments
â”‚   â”œâ”€â”€ dataset_catpred_ki/          # CatPred Ki experiments
â”‚   â”œâ”€â”€ dataset_catpred_km/          # CatPred Km experiments
â”‚   â”œâ”€â”€ dataset_EITLEM_kcat/         # EITLEM kcat experiments
â”‚   â”œâ”€â”€ dataset_EITLEM_kkm/          # EITLEM kkm experiments
â”‚   â”œâ”€â”€ dataset_EITLEM_km/           # EITLEM Km experiments
â”‚   â”œâ”€â”€ dataset_inhouse_kd/          # In-house Kd experiments
â”‚   â”œâ”€â”€ dataset_MPEK_kcat/           # MPEK kcat experiments
â”‚   â””â”€â”€ dataset_MPEK_km/             # MPEK Km experiments
â”œâ”€â”€ presentation/                     # Project presentations and figures
â”œâ”€â”€ ref/                             # Reference implementations and papers
â”‚   â”œâ”€â”€ CatPred-1.0.1/              # CatPred reference implementation
â”‚   â”œâ”€â”€ EITLEM-Kinetics/             # EITLEM reference code
â”‚   â””â”€â”€ mutadescrib/                 # MutaDescribe reference
â””â”€â”€ .history/                        # Version history
```

## ğŸ“Š Datasets

The project includes experiments on multiple datasets:

- **CatPred**: Comprehensive enzyme kinetic database for kcat, Km, and Ki
- **EITLEM**: Enzyme kinetic parameters dataset
- **MPEK**: Metabolic pathway enzyme kinetics
- **In-house Kd**: Custom binding affinity dataset

Each dataset includes both cold protein and cold molecule splitting strategies for robust evaluation.

## ğŸ¤– Models

### Traditional ML Models
- **Linear Regression (LR)**: Baseline linear model
- **Support Vector Regression (SVR)**: Non-linear regression with RBF kernel
- **Random Forest (RF)**: Ensemble tree-based method
- **XGBoost (XGB)**: Gradient boosting framework
- **Gradient Boosting Machine (GBM)**: Scikit-learn gradient boosting
- **CatBoost (CAT)**: Categorical boosting algorithm

### Deep Learning Models
- **Multi-Layer Perceptron (MLP)**: Fully connected neural network
- **Convolutional Neural Network (CNN)**: 1D convolution for sequence data
- **Transformer (TRANS)**: Attention-based architecture
- **Diffusion Models**: Generative approach for enzyme kinetics

## âš™ï¸ Installation

### Prerequisites
- Python 3.8+
- conda or miniconda
- CUDA-capable GPU (recommended for deep learning models)

### Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd Huang_lab/repo

# Create conda environment
conda create -n enzyme_kinetics python=3.8
conda activate enzyme_kinetics

# Install required packages
pip install -r requirements.txt

# For GPU support (optional)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Required Packages
```
pandas
numpy
scikit-learn
torch
xgboost
catboost
joblib
tqdm
matplotlib
seaborn
```

## ğŸš€ Usage

### Running Individual Models

Navigate to the specific experiment directory and run the desired model:

```bash
cd experiments/dataset_catpred_kcat/A02_code/cold_split/

# Run SVR model
jupyter notebook ER_RegressionModel_SVR_cold.ipynb

# Run deep learning models
jupyter notebook ER_RegressionModel_CNN_cold.ipynb
jupyter notebook ER_RegressionModel_MLP_cold.ipynb
```

### Model Training and Evaluation

Each notebook contains:
1. Data preprocessing and feature engineering
2. Model training with hyperparameter tuning
3. Evaluation on test sets
4. Performance metrics calculation
5. Model saving for future use

### Loading Pre-trained Models

```python
from joblib import load

# Load traditional ML models
model = load('path/to/saved_model.joblib')

# Load deep learning models
import torch
model = YourModelClass()
model.load_state_dict(torch.load('path/to/model.pt'))
```

## ğŸ§ª Experiments

### Data Splitting Strategies

1. **Cold Protein Split**: Test on unseen proteins
2. **Cold Molecule Split**: Test on unseen molecules
3. **Random Split**: Standard random train/test split

### Evaluation Metrics

- **RÂ²**: Coefficient of determination
- **MAE**: Mean Absolute Error
- **Pearson Correlation**: Linear correlation coefficient
- **Median AE**: Median Absolute Error
- **Explained Variance**: Fraction of variance explained

### Performance Tracking

Results are automatically saved to CSV files:
```
model performance metrics_Catpred.csv
model performance metrics.csv
```

## ğŸ“ˆ Results

Model performance is evaluated across different datasets and splitting strategies. Key findings include:

- Performance comparison between traditional ML and deep learning approaches
- Impact of cold protein vs. cold molecule splitting on generalization
- Dataset-specific model performance variations

Detailed results and analysis can be found in the presentation files and individual experiment notebooks.

## ğŸ”„ Reproducibility

To reproduce the results:

1. Ensure all dependencies are installed
2. Download the required datasets (contact repository maintainers)
3. Run the notebooks in the appropriate experiment directories
4. Results will be saved automatically to the specified output directories

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-model`)
3. Commit your changes (`git commit -am 'Add new model'`)
4. Push to the branch (`git push origin feature/new-model`)
5. Create a Pull Request

## ğŸ“š References

- [CatPred: A Comprehensive Framework for Deep Learning In Vitro Enzyme Kinetic Parameters](https://github.com/maranasgroup/catpred)
- [EITLEM: Enzyme Kinetics Prediction](ref/EITLEM-Kinetics/)
- [UniKP and DLKcat implementations](ref/CatPred-1.0.1/)

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“§ Contact

For questions and collaborations, please contact the Huang Lab research team.

---

**Note**: This repository contains experimental code for research purposes. Please ensure proper validation before using models in production environments.