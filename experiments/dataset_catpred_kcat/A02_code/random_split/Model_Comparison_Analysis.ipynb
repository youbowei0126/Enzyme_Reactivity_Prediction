{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1069f5c",
   "metadata": {},
   "source": [
    "# Model Comparison Analysis: fit() vs load_from_file()\n",
    "\n",
    "This notebook investigates the discrepancy between training a Linear Regression model using `ModelBenchmark.fit()` versus loading a pre-trained model with `load_from_file()`.\n",
    "\n",
    "## Expected vs Actual Results\n",
    "- **Expected**: Identical predictions (theory)\n",
    "- **Actual**: Small numerical differences observed\n",
    "- **Hypothesis**: Differences in data preprocessing, random seeds, or model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449d442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(init=True, repr=True, eq=False, order=False, unsafe_hash=False, frozen=False)\n",
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    A comprehensive benchmarking class for evaluating different machine learning models\n",
    "    on enzyme kinetic datasets with various data splitting strategies.\n",
    "    \n",
    "    This class encapsulates the entire machine learning pipeline including data preprocessing,\n",
    "    train/validation/test splitting, model initialization, training, and prediction.\n",
    "    Supports multiple traditional ML algorithms and data splitting methods for robust evaluation.\n",
    "    \n",
    "    Attributes:\n",
    "        data (pd.DataFrame | str): Input dataset as DataFrame or path to joblib file\n",
    "        model_type (str): Type of model to use ('CAT', 'GBM', 'LR', 'RF', 'SVR', 'XGB')\n",
    "        split_method (str): Data splitting strategy ('random', 'cold_mols', 'cold_proteins', 'load_from_file')\n",
    "    \n",
    "    Methods:\n",
    "        __post_init: Post-initialization to load and validate input data\n",
    "        rename_data_columns: Rename DataFrame columns using a mapping dictionary\n",
    "        data_preprocessing: Preprocess features and labels for model training\n",
    "        split_data: Split dataset into train/validation/test sets\n",
    "        model_init: Initialize the specified model type\n",
    "        fit: Train the model on training data\n",
    "        predict: Generate predictions using the trained model\n",
    "    \"\"\"\n",
    "    data: pd.DataFrame | str\n",
    "    model_type: str\n",
    "    split_method: str\n",
    "\n",
    "\n",
    "    '''\n",
    "    def __init__(self, data, model_type, split_method, *args, **kwargs):\n",
    "        \"\"\"Initialize the BenchMarkModel with data, model type, and split method.\"\"\"\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Data must be a pandas DataFrame.\")\n",
    "        self.data = data\n",
    "        self.model_type = model_type\n",
    "        self.split_method = split_method\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs'''\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method to load and validate input data.\n",
    "        \n",
    "        Handles both DataFrame objects and file paths to joblib files.\n",
    "        Ensures data integrity and proper format for downstream processing.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If data type is unsupported or file loading fails\n",
    "        \"\"\"\n",
    "        # Debug information for data type checking\n",
    "        print(f\"Initial data type: {type(self.data)}\")\n",
    "        \n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            print(\"Data is already a DataFrame\")\n",
    "            # Create a copy to avoid modifying original data\n",
    "            self.data = self.data.copy()\n",
    "        elif isinstance(self.data, str):\n",
    "            print(f\"Loading data from: {self.data}\")\n",
    "            try:\n",
    "                # Load data from joblib file\n",
    "                self.data = joblib.load(self.data)\n",
    "                print(f\"Data loaded successfully, type: {type(self.data)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data: {e}\")\n",
    "                raise ValueError(f\"Could not load data from {self.data}: {e}\")\n",
    "        else:\n",
    "            raise ValueError(\"Data must be a pandas DataFrame or a valid file path.\")\n",
    "        \n",
    "        \n",
    "        if self.split_method==\"random_split\":\n",
    "            self.split_method = \"random\"\n",
    "        elif self.split_method == \"cold_molecules\":\n",
    "            self.split_method = \"cold_mols\"\n",
    "        elif self.split_method not in [\"random\", \"cold_mols\", \"cold_proteins\", \"load_index_from_file\"]:\n",
    "            raise ValueError(f\"Unknown split method: {self.split_method}\")\n",
    "        \n",
    "    \n",
    "    def rename_data_columns(self, rename_dict):\n",
    "        \"\"\"\n",
    "        Rename DataFrame columns using the provided mapping dictionary.\n",
    "        \n",
    "        Args:\n",
    "            rename_dict (dict): Dictionary mapping old column names to new ones\n",
    "            \n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "        \"\"\"\n",
    "        self.data = self.data.rename(columns=rename_dict)\n",
    "        return self\n",
    "\n",
    "    def data_preprocessing(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset by preparing features and labels for model training.\n",
    "        \n",
    "        This method handles column renaming, feature concatenation, and label extraction.\n",
    "        Currently supports traditional ML models that require concatenated features.\n",
    "        \n",
    "        Args:\n",
    "            *args: Variable length argument list\n",
    "            **kwargs: Arbitrary keyword arguments including:\n",
    "                rename_dict (dict, optional): Column renaming mapping\n",
    "                \n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If required feature columns are missing\n",
    "        \"\"\"\n",
    "        # Apply column renaming if specified\n",
    "        if kwargs.get(\"rename_dict\"):\n",
    "            self.rename_data_columns(kwargs[\"rename_dict\"])\n",
    "\n",
    "        # Validate required columns exist\n",
    "        if (\"metabolite_features\" not in self.data.columns) or (\"protein_features\" not in self.data.columns):\n",
    "            raise ValueError(\"Data is missing required feature columns or wrong column name.\")\n",
    "        \n",
    "        # Process data for traditional ML models\n",
    "        if self.model_type in [\"CAT\", \"GBM\", \"LR\", \"RF\", \"SVR\", \"XGB\"]:\n",
    "            # Concatenate metabolite and protein features into a single feature vector\n",
    "            # This creates a flat feature representation suitable for traditional ML algorithms\n",
    "            self.X = np.array([np.concatenate([m, p]) for m, p in zip(self.data[\"metabolite_features\"], self.data[\"protein_features\"])])\n",
    "            self.y = self.data[\"label\"]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Split the dataset into training, validation, and test sets based on the specified method.\n",
    "        \n",
    "        Supports multiple splitting strategies:\n",
    "        - Random split: Standard random partitioning (70% train, 15% val, 15% test)\n",
    "        - Cold molecules: Split by unique molecules (to be implemented)\n",
    "        - Cold proteins: Split by unique proteins (to be implemented)\n",
    "        - Load from file: Use pre-computed split indices\n",
    "        \n",
    "        Args:\n",
    "            *args: Variable length argument list\n",
    "            **kwargs: Arbitrary keyword arguments including:\n",
    "                save_index_path (str, optional): Path to save split indices\n",
    "                index_file_path (str, required for load_index_from_file): Path to load pre-computed indices\n",
    "\n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If split method is unknown or indices don't match data length\n",
    "        \"\"\"\n",
    "        # Generate train/validation/test indices based on split method\n",
    "        if self.split_method == \"random\" or self.split_method == \"random_split\":\n",
    "            # Standard random split: 70% train, 15% validation, 15% test\n",
    "            self.train_index, self.temp_index = train_test_split(np.arange(len(self.data)),test_size=0.3, shuffle=True, random_state=42)\n",
    "            self.val_index, self.test_index = train_test_split(self.temp_index, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "        elif self.split_method == \"cold_mols\":\n",
    "            # TODO: Implement cold molecules split logic\n",
    "            # This would split by unique molecules to test generalization to new compounds\n",
    "            pass\n",
    "        elif self.split_method == \"cold_proteins\":\n",
    "            # TODO: Implement cold proteins split logic  \n",
    "            # This would split by unique proteins to test generalization to new enzymes\n",
    "            pass\n",
    "        elif self.split_method == \"load_index_from_file\":\n",
    "            # Load pre-computed split indices from JSON file\n",
    "            if \"index_file_path\" not in kwargs:\n",
    "                raise ValueError(\"index_file_path must be provided for load_index_from_file split method.\")\n",
    "            with open(kwargs[\"index_file_path\"], \"r\") as f:\n",
    "                indices = json.load(f)\n",
    "                self.train_index = np.array(indices[\"train_index\"])\n",
    "                self.val_index = np.array(indices[\"val_index\"])\n",
    "                self.test_index = np.array(indices[\"test_index\"])\n",
    "                \n",
    "            # Validate that indices cover all data points exactly once\n",
    "            if len(self.train_index)+ len(self.val_index) + len(self.test_index) != len(self.data):\n",
    "                raise ValueError(\"Indices from file do not match the length of the data.\")   \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split method: {self.split_method}\")\n",
    "        \n",
    "        # Create feature and label subsets for each split\n",
    "        self.train_X = self.X[self.train_index]\n",
    "        self.train_y = self.y[self.train_index]\n",
    "        self.val_X = self.X[self.val_index]\n",
    "        self.val_y = self.y[self.val_index]\n",
    "        self.test_X = self.X[self.test_index]\n",
    "        self.test_y = self.y[self.test_index]\n",
    "\n",
    "        # Save split indices to file if specified\n",
    "        if kwargs.get(\"save_index_path\"):\n",
    "            with open(kwargs[\"save_index_path\"], \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"train_index\": self.train_index.tolist(),\n",
    "                    \"val_index\": self.val_index.tolist(),\n",
    "                    \"test_index\": self.test_index.tolist()\n",
    "                }, f)\n",
    "        return self\n",
    "    \n",
    "    def model_init(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the specified machine learning model with given parameters.\n",
    "        \n",
    "        Supports various traditional ML algorithms including tree-based methods,\n",
    "        linear models, and support vector machines.\n",
    "        \n",
    "        Args:\n",
    "            *args: Variable length argument list passed to model constructor\n",
    "            **kwargs: Arbitrary keyword arguments passed to model constructor\n",
    "            \n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            ImportError: If required model library is not installed\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.model_type}(traditional) model...\")\n",
    "        \n",
    "        # Initialize model based on specified type\n",
    "        if self.model_type == \"CAT\":\n",
    "            # CatBoost Gradient Boosting\n",
    "            import catboost as cat\n",
    "            self.model = cat.CatBoostRegressor( *args, **kwargs)\n",
    "        elif self.model_type == \"GBM\":\n",
    "            # Scikit-learn Gradient Boosting\n",
    "            from sklearn.ensemble import GradientBoostingRegressor\n",
    "            self.model = GradientBoostingRegressor( *args, **kwargs)\n",
    "        elif self.model_type == \"LR\":\n",
    "            # Linear Regression\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            self.model = LinearRegression( *args, **kwargs)\n",
    "        elif self.model_type == \"RF\":\n",
    "            # Random Forest with fixed random state for reproducibility\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            self.model = RandomForestRegressor(random_state=42)\n",
    "        elif self.model_type == \"SVR\":\n",
    "            # Support Vector Regression\n",
    "            from sklearn.svm import SVR\n",
    "            self.model = SVR( *args, **kwargs)\n",
    "        elif self.model_type == \"XGB\":\n",
    "            # XGBoost Gradient Boosting\n",
    "            import xgboost as xg\n",
    "            self.model = xg.XGBRegressor( *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Train the initialized model on the training dataset.\n",
    "        \n",
    "        Uses the preprocessed training features (train_X) and labels (train_y)\n",
    "        to fit the model parameters.\n",
    "        \n",
    "        Args:\n",
    "            *args: Variable length argument list passed to model.fit()\n",
    "            **kwargs: Arbitrary keyword arguments passed to model.fit()\n",
    "            \n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if self.model_type in [\"CAT\", \"GBM\", \"LR\", \"RF\", \"SVR\", \"XGB\"]:\n",
    "            # Train the model using standard scikit-learn API\n",
    "            self.model.fit(self.train_X, self.train_y, *args, **kwargs)\n",
    "            return self\n",
    "        else:\n",
    "            \"\"\"\n",
    "            TODO: Implement training logic for other model types\n",
    "            \"\"\"\n",
    "            pass\n",
    "    \n",
    "    def load_model_from_file(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model from file.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the saved model file\n",
    "            \n",
    "        Returns:\n",
    "            ModelBenchmark: Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if self.model_type in [\"CAT\", \"GBM\", \"LR\", \"RF\", \"SVR\", \"XGB\"]:\n",
    "            self.model = joblib.load(model_path)\n",
    "            print(f\"Model loaded from: {model_path}\")\n",
    "            return self\n",
    "        else:\n",
    "            \"\"\"\n",
    "            TODO: Implement model loading logic for other model types\n",
    "            \"\"\"\n",
    "            pass\n",
    "    \n",
    "    def save_model_to_file(self, model_path):\n",
    "        \"\"\"\n",
    "        Save the trained model to file.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path where to save the model\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'model'):\n",
    "            joblib.dump(self.model, model_path)\n",
    "            print(f\"Model saved to: {model_path}\")\n",
    "        else:\n",
    "            raise ValueError(\"No model to save. Please train a model first.\")\n",
    "    \n",
    "    def compare_predictions(self, other_predictions, test_data=None):\n",
    "        \"\"\"\n",
    "        Compare predictions with another set of predictions.\n",
    "        \n",
    "        Args:\n",
    "            other_predictions (numpy.ndarray): Other predictions to compare with\n",
    "            test_data (numpy.ndarray, optional): Test data to use for prediction\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing comparison metrics\n",
    "        \"\"\"\n",
    "        if test_data is None:\n",
    "            test_data = self.test_X\n",
    "            \n",
    "        current_predictions = self.predict(test_data)\n",
    "        \n",
    "        # Calculate differences\n",
    "        abs_diff = np.abs(current_predictions - other_predictions)\n",
    "        \n",
    "        comparison_metrics = {\n",
    "            'max_absolute_difference': np.max(abs_diff),\n",
    "            'mean_absolute_difference': np.mean(abs_diff),\n",
    "            'std_absolute_difference': np.std(abs_diff),\n",
    "            'median_absolute_difference': np.median(abs_diff),\n",
    "            'rmse_difference': np.sqrt(np.mean(abs_diff**2)),\n",
    "            'are_identical': np.allclose(current_predictions, other_predictions, rtol=1e-10, atol=1e-10)\n",
    "        }\n",
    "        \n",
    "        return comparison_metrics\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate predictions using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input features for prediction\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Model predictions\n",
    "        \"\"\"\n",
    "        if self.model_type in [\"CAT\", \"GBM\", \"LR\", \"RF\", \"SVR\", \"XGB\"]:\n",
    "            return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36690a9d",
   "metadata": {},
   "source": [
    "## Method 1: Using ModelBenchmark.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a979e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Method 1: ModelBenchmark.fit() ===\n",
      "Initial data type: <class 'str'>\n",
      "Loading data from: ./../../A01_dataset/kcat_with_features.joblib\n",
      "Data loaded successfully, type: <class 'pandas.core.frame.DataFrame'>\n",
      "Training LR(traditional) model...\n",
      "Data loaded successfully, type: <class 'pandas.core.frame.DataFrame'>\n",
      "Training LR(traditional) model...\n",
      "Method 1 predictions shape: (3473,)\n",
      "Method 1 first 10 predictions: [ 0.8442364   0.9761486   0.46442795  1.6891556   0.239748    1.2012768\n",
      "  1.1019135   0.8296261   0.6127968  -1.7968616 ]\n",
      "Method 1 predictions shape: (3473,)\n",
      "Method 1 first 10 predictions: [ 0.8442364   0.9761486   0.46442795  1.6891556   0.239748    1.2012768\n",
      "  1.1019135   0.8296261   0.6127968  -1.7968616 ]\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using ModelBenchmark class with fit()\n",
    "print(\"=== Method 1: ModelBenchmark.fit() ===\")\n",
    "benchmark_model = ModelBenchmark(\n",
    "    data=\"./../../A01_dataset/kcat_with_features.joblib\",\n",
    "    model_type=\"LR\",\n",
    "    split_method=\"random\"\n",
    ")\n",
    "\n",
    "# Preprocess and train\n",
    "benchmark_model.data_preprocessing(rename_dict={\"log10kcat_max\": \"label\"})\n",
    "benchmark_model.split_data()\n",
    "benchmark_model.model_init()\n",
    "benchmark_model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions_method1 = benchmark_model.predict(benchmark_model.test_X)\n",
    "print(f\"Method 1 predictions shape: {predictions_method1.shape}\")\n",
    "print(f\"Method 1 first 10 predictions: {predictions_method1[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04422a",
   "metadata": {},
   "source": [
    "## Method 2: Manual Implementation (Original ER_RegressionModel_LR approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4804154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 2: Manual Implementation ===\n",
      "Method 2 predictions shape: (3473,)\n",
      "Method 2 first 10 predictions: [ 0.8442364   0.9761486   0.46442795  1.6891556   0.239748    1.2012768\n",
      "  1.1019135   0.8296261   0.6127968  -1.7968616 ]\n",
      "Method 2 predictions shape: (3473,)\n",
      "Method 2 first 10 predictions: [ 0.8442364   0.9761486   0.46442795  1.6891556   0.239748    1.2012768\n",
      "  1.1019135   0.8296261   0.6127968  -1.7968616 ]\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Manual implementation matching ER_RegressionModel_LR.ipynb\n",
    "print(\"\\n=== Method 2: Manual Implementation ===\")\n",
    "\n",
    "# Load and preprocess data exactly as in ER_RegressionModel_LR.ipynb\n",
    "data_kcat = joblib.load('./../../A01_dataset/kcat_with_features.joblib')\n",
    "data_kcat.rename(columns={'log10kcat_max':'label'}, inplace=True)\n",
    "\n",
    "# Split data with identical parameters\n",
    "train_df, temp_df = train_test_split(data_kcat, test_size=0.3, shuffle=True, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare features exactly as in original\n",
    "train_X_manual = np.array([\n",
    "    np.concatenate([m, p])\n",
    "    for m, p in zip(train_df['metabolite_features'], train_df['protein_features'])\n",
    "])\n",
    "train_y_manual = train_df['label']\n",
    "\n",
    "test_X_manual = np.array([\n",
    "    np.concatenate([m, p])\n",
    "    for m, p in zip(test_df['metabolite_features'], test_df['protein_features'])\n",
    "])\n",
    "test_y_manual = test_df['label']\n",
    "\n",
    "# Train model\n",
    "lr_manual = LinearRegression()\n",
    "lr_manual.fit(train_X_manual, train_y_manual)\n",
    "\n",
    "# Generate predictions\n",
    "predictions_method2 = lr_manual.predict(test_X_manual)\n",
    "print(f\"Method 2 predictions shape: {predictions_method2.shape}\")\n",
    "print(f\"Method 2 first 10 predictions: {predictions_method2[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b970e5",
   "metadata": {},
   "source": [
    "## Method 3: Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58d05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 3: Load Pre-trained Model ===\n",
      "Method 3 predictions shape: (3473,)\n",
      "Method 3 first 10 predictions: [ 0.8463745   0.9789429   0.4663391   1.6912537   0.24151611  1.203186\n",
      "  1.1044312   0.83166504  0.6152954  -1.7943726 ]\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Load the saved model from ER_RegressionModel_LR.ipynb\n",
    "print(\"\\n=== Method 3: Load Pre-trained Model ===\")\n",
    "\n",
    "try:\n",
    "    # Try to load the model (update path as needed)\n",
    "    loaded_model = joblib.load('./../../A03_models/random_split/LR model_Catpred.joblib')\n",
    "    \n",
    "    # Use the same test data as Method 2 for fair comparison\n",
    "    predictions_method3 = loaded_model.predict(test_X_manual)\n",
    "    print(f\"Method 3 predictions shape: {predictions_method3.shape}\")\n",
    "    print(f\"Method 3 first 10 predictions: {predictions_method3[:10]}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-trained model file not found. Creating and saving a model for comparison...\")\n",
    "    # Save the manually trained model for consistency\n",
    "    joblib.dump(lr_manual, './temp_lr_model.joblib')\n",
    "    loaded_model = joblib.load('./temp_lr_model.joblib')\n",
    "    predictions_method3 = loaded_model.predict(test_X_manual)\n",
    "    print(f\"Method 3 predictions shape: {predictions_method3.shape}\")\n",
    "    print(f\"Method 3 first 10 predictions: {predictions_method3[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d418fa",
   "metadata": {},
   "source": [
    "## Detailed Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaf043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Shape Comparison ===\n",
      "Method 1 test_X shape: (3473, 1088)\n",
      "Method 2 test_X shape: (3473, 1088)\n",
      "Method 1 test_y shape: (3473,)\n",
      "Method 2 test_y shape: (3473,)\n",
      "\n",
      "=== Index Comparison ===\n",
      "Method 1 test indices (first 10): [14384 20750 14099  1522 21680 18656 20026 13459 14465  5500]\n",
      "Method 2 test indices (first 10): [14384 20750 14099  1522 21680 18656 20026 13459 14465  5500]\n",
      "\n",
      "=== Feature Statistics Comparison ===\n",
      "Method 1 test_X mean: -0.003720\n",
      "Method 2 test_X mean: -0.003720\n",
      "Method 1 test_X std: 0.490411\n",
      "Method 2 test_X std: 0.490411\n"
     ]
    }
   ],
   "source": [
    "# Compare data shapes and basic statistics\n",
    "print(\"=== Data Shape Comparison ===\")\n",
    "print(f\"Method 1 test_X shape: {benchmark_model.test_X.shape}\")\n",
    "print(f\"Method 2 test_X shape: {test_X_manual.shape}\")\n",
    "print(f\"Method 1 test_y shape: {benchmark_model.test_y.shape}\")\n",
    "print(f\"Method 2 test_y shape: {test_y_manual.shape}\")\n",
    "\n",
    "print(\"\\n=== Index Comparison ===\")\n",
    "print(f\"Method 1 test indices (first 10): {benchmark_model.test_index[:10]}\")\n",
    "print(f\"Method 2 test indices (first 10): {test_df.index[:10].values}\")\n",
    "\n",
    "print(\"\\n=== Feature Statistics Comparison ===\")\n",
    "print(f\"Method 1 test_X mean: {benchmark_model.test_X.mean():.6f}\")\n",
    "print(f\"Method 2 test_X mean: {test_X_manual.mean():.6f}\")\n",
    "print(f\"Method 1 test_X std: {benchmark_model.test_X.std():.6f}\")\n",
    "print(f\"Method 2 test_X std: {test_X_manual.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80969ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Identity Check ===\n",
      "Test sets identical: True\n"
     ]
    }
   ],
   "source": [
    "# Check if test sets are identical\n",
    "print(\"=== Test Set Identity Check ===\")\n",
    "\n",
    "# Compare a few sample features\n",
    "if benchmark_model.test_X.shape == test_X_manual.shape:\n",
    "    # Check if the data is identical\n",
    "    are_identical = np.allclose(benchmark_model.test_X, test_X_manual, rtol=1e-10, atol=1e-10)\n",
    "    print(f\"Test sets identical: {are_identical}\")\n",
    "    \n",
    "    if not are_identical:\n",
    "        # Find differences\n",
    "        diff_mask = ~np.isclose(benchmark_model.test_X, test_X_manual, rtol=1e-10, atol=1e-10)\n",
    "        num_differences = np.sum(diff_mask)\n",
    "        print(f\"Number of different elements: {num_differences}\")\n",
    "        print(f\"Percentage different: {num_differences / benchmark_model.test_X.size * 100:.4f}%\")\n",
    "        \n",
    "        if num_differences > 0:\n",
    "            # Show some example differences\n",
    "            diff_indices = np.where(diff_mask)\n",
    "            for i in range(min(5, len(diff_indices[0]))):\n",
    "                row, col = diff_indices[0][i], diff_indices[1][i]\n",
    "                print(f\"Difference at [{row}, {col}]: {benchmark_model.test_X[row, col]} vs {test_X_manual[row, col]}\")\n",
    "else:\n",
    "    print(\"Test sets have different shapes - cannot compare directly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec3e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Coefficients Comparison ===\n",
      "Method 1 coefficients shape: (1088,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Model Coefficients Comparison ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMethod 1 coefficients shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_model.model.coef_.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMethod 2 coefficients shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_manual.model.coef_.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMethod 3 coefficients shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloaded_model.coef_.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMethod 1 intercept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_model.model.intercept_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'LinearRegression' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "# Compare model coefficients\n",
    "print(\"=== Model Coefficients Comparison ===\")\n",
    "print(f\"Method 1 coefficients shape: {benchmark_model.model.coef_.shape}\")\n",
    "print(f\"Method 2 coefficients shape: {lr_manual.model.coef_.shape}\")\n",
    "print(f\"Method 3 coefficients shape: {loaded_model.coef_.shape}\")\n",
    "\n",
    "print(f\"\\nMethod 1 intercept: {benchmark_model.model.intercept_:.10f}\")\n",
    "print(f\"Method 2 intercept: {lr_manual.intercept_:.10f}\")\n",
    "print(f\"Method 3 intercept: {loaded_model.intercept_:.10f}\")\n",
    "\n",
    "# Check coefficient differences\n",
    "coeff_diff_1_2 = np.abs(benchmark_model.model.coef_ - lr_manual.coef_)\n",
    "coeff_diff_2_3 = np.abs(lr_manual.coef_ - loaded_model.coef_)\n",
    "\n",
    "print(f\"\\nMax coefficient difference (Method 1 vs 2): {np.max(coeff_diff_1_2):.2e}\")\n",
    "print(f\"Max coefficient difference (Method 2 vs 3): {np.max(coeff_diff_2_3):.2e}\")\n",
    "print(f\"Mean coefficient difference (Method 1 vs 2): {np.mean(coeff_diff_1_2):.2e}\")\n",
    "print(f\"Mean coefficient difference (Method 2 vs 3): {np.mean(coeff_diff_2_3):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions numerically\n",
    "print(\"=== Prediction Comparison ===\")\n",
    "\n",
    "# For fair comparison, use Method 2's test set for all predictions\n",
    "pred_1_on_manual_test = benchmark_model.model.predict(test_X_manual)\n",
    "pred_2 = predictions_method2\n",
    "pred_3 = predictions_method3\n",
    "\n",
    "print(f\"Method 1 on manual test set (first 10): {pred_1_on_manual_test[:10]}\")\n",
    "print(f\"Method 2 predictions (first 10): {pred_2[:10]}\")\n",
    "print(f\"Method 3 predictions (first 10): {pred_3[:10]}\")\n",
    "\n",
    "# Calculate differences\n",
    "diff_1_2 = np.abs(pred_1_on_manual_test - pred_2)\n",
    "diff_2_3 = np.abs(pred_2 - pred_3)\n",
    "diff_1_3 = np.abs(pred_1_on_manual_test - pred_3)\n",
    "\n",
    "print(f\"\\nMax difference (Method 1 vs 2): {np.max(diff_1_2):.2e}\")\n",
    "print(f\"Max difference (Method 2 vs 3): {np.max(diff_2_3):.2e}\")\n",
    "print(f\"Max difference (Method 1 vs 3): {np.max(diff_1_3):.2e}\")\n",
    "\n",
    "print(f\"Mean difference (Method 1 vs 2): {np.mean(diff_1_2):.2e}\")\n",
    "print(f\"Mean difference (Method 2 vs 3): {np.mean(diff_2_3):.2e}\")\n",
    "print(f\"Mean difference (Method 1 vs 3): {np.mean(diff_1_3):.2e}\")\n",
    "\n",
    "print(f\"Std difference (Method 1 vs 2): {np.std(diff_1_2):.2e}\")\n",
    "print(f\"Std difference (Method 2 vs 3): {np.std(diff_2_3):.2e}\")\n",
    "print(f\"Std difference (Method 1 vs 3): {np.std(diff_1_3):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scatter plot: Method 1 vs Method 2\n",
    "axes[0, 0].scatter(pred_1_on_manual_test, pred_2, alpha=0.6)\n",
    "axes[0, 0].plot([pred_2.min(), pred_2.max()], [pred_2.min(), pred_2.max()], 'r--')\n",
    "axes[0, 0].set_xlabel('Method 1 Predictions')\n",
    "axes[0, 0].set_ylabel('Method 2 Predictions')\n",
    "axes[0, 0].set_title('Method 1 vs Method 2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Method 2 vs Method 3\n",
    "axes[0, 1].scatter(pred_2, pred_3, alpha=0.6)\n",
    "axes[0, 1].plot([pred_2.min(), pred_2.max()], [pred_2.min(), pred_2.max()], 'r--')\n",
    "axes[0, 1].set_xlabel('Method 2 Predictions')\n",
    "axes[0, 1].set_ylabel('Method 3 Predictions')\n",
    "axes[0, 1].set_title('Method 2 vs Method 3')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Difference histogram: Method 1 vs Method 2\n",
    "axes[1, 0].hist(diff_1_2, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Absolute Difference')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Difference Distribution: Method 1 vs Method 2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Difference histogram: Method 2 vs Method 3\n",
    "axes[1, 1].hist(diff_2_3, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Absolute Difference')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Difference Distribution: Method 2 vs Method 3')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575ae48",
   "metadata": {},
   "source": [
    "## Potential Sources of Discrepancy\n",
    "\n",
    "Based on the analysis above, potential sources of numerical differences include:\n",
    "\n",
    "1. **Data Type Precision**: Different numpy data types (float32 vs float64)\n",
    "2. **Feature Engineering**: Slight differences in how features are concatenated\n",
    "3. **Model Serialization**: Precision loss during joblib save/load operations\n",
    "4. **Random Seed Propagation**: Different random states affecting data processing\n",
    "5. **Memory Layout**: Different array memory layouts affecting numerical computation\n",
    "6. **Sklearn Version**: Different sklearn versions with slightly different algorithms\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Ensure Identical Data Processing**: Use the same data preprocessing pipeline\n",
    "2. **Check Data Types**: Ensure all arrays use the same precision (float64)\n",
    "3. **Verify Random Seeds**: Set random seeds consistently across all methods\n",
    "4. **Model Serialization**: Consider the precision limits of joblib serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional diagnostic: Check data types and memory layout\n",
    "print(\"=== Data Type and Memory Layout Analysis ===\")\n",
    "print(f\"Method 1 test_X dtype: {benchmark_model.test_X.dtype}\")\n",
    "print(f\"Method 2 test_X dtype: {test_X_manual.dtype}\")\n",
    "print(f\"Method 1 test_X memory layout: {benchmark_model.test_X.flags}\")\n",
    "print(f\"Method 2 test_X memory layout: {test_X_manual.flags}\")\n",
    "\n",
    "# Check if converting to same dtype reduces differences\n",
    "if benchmark_model.test_X.dtype != test_X_manual.dtype:\n",
    "    print(f\"\\nConverting both to float64 for comparison...\")\n",
    "    test_X_method1_f64 = benchmark_model.test_X.astype(np.float64)\n",
    "    test_X_method2_f64 = test_X_manual.astype(np.float64)\n",
    "    \n",
    "    pred_1_f64 = benchmark_model.model.predict(test_X_method1_f64)\n",
    "    pred_2_f64 = lr_manual.predict(test_X_method2_f64)\n",
    "    \n",
    "    print(f\"Difference after dtype conversion: {np.max(np.abs(pred_1_f64 - pred_2_f64)):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
